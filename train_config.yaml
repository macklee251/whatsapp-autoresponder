# === MODELO BASE ===
model_name_or_path: /workspace/qwen2.5-7b-instruct
trust_remote_code: true

# === TIPO DE FINE-TUNE ===
finetuning_type: lora
lora_target: all
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05

# === DADOS ===
dataset: sharegpt
dataset_dir: /workspace/whatsapp-autoresponder/data
dataset_format: sharegpt
# LLaMA-Factory vai carregar todos os *.jsonl do dataset_dir que estiverem no formato ShareGPT

# Divide 2% para validação a partir do treino
val_size: 0.02
preprocessing_num_workers: 4

# === TREINO ===
output_dir: /workspace/output/lora-qwen2.5-7b
logging_dir: /workspace/logs
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-5
weight_decay: 0.0
warmup_ratio: 0.03
lr_scheduler_type: cosine
max_grad_norm: 1.0
seed: 42

# === GERAÇÃO / CONTEXTO ===
cutoff_len: 2048
packing: true
train_on_prompt: false

# === PRECISÃO ===
fp16: true

# === LOG / CHECKPOINTS ===
logging_steps: 25
save_steps: 500
save_total_limit: 3
eval_steps: 500

# === OUTROS ===
report_to: none